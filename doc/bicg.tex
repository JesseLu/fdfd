\documentclass{article}
\usepackage{amsmath}
\usepackage{hyperref}
\title{Notes on the Bi-Conjugate Gradient Method}
\author{Jesse Lu}
\begin{document}
\maketitle
\tableofcontents
\section{The algorithm}
The algorithm attempts to solve $Ax = b$,
    where the matrix $A$ is indefinite.
The algorithm accepts as inputs
    \begin{enumerate}
    \item a method to multiply a vector by $A$,
    \item a method to multiply a vector by $A^T$ 
        (note that this is the transpose of $A$, 
        \emph{not} its conjugate-transpose), and
    \item the vector $b$.
    \end{enumerate}

The algorithm begins by initializing the following variables,
    \begin{subequations}\begin{align}
    r_0 &= b - Ax_0 \\
    \hat{r_0} &= b - A^T \hat{x_0} \\
    p_0 &= r_0 \\
    \hat{p_0} &= \hat{r_0}.
    \end{align}\end{subequations}
    \begin{subequations}\begin{align}
    \end{align}\end{subequations}


\appendix
\section{Reference algorithm: From C. T. Kelley}
Found in section 3.6.1 of \cite{kelley}.
Initialize as follows:
\begin{equation}
r = b - Ax, \hat{r} = r, \rho_0 = 1, \hat{p} = p = 0, k = 0.
\end{equation}
Repeat the following until a termination condition 
    such as $\|r\|_2 < \epsilon \|b\|_2$ is satisfied,
    \begin{subequations}\begin{align}
    k &= k + 1 \\
    \rho_k &= \hat{r}^T r, \beta = \rho_k / \rho_{k-1} \\
    p &= r + \beta p, \hat{p} = \hat{r} + \beta \hat{p} \\
    v &= Ap \\
    \alpha &= \rho_k / (\hat{p}^T v) \\
    x &= x + \alpha p \\
    r &= r - \alpha v, \hat{r} = \hat{r} - \alpha A^T \hat{p}.
    \end{align}\end{subequations}

\section{Reference algorithm: From Wikipedia}
First, choose initial vectors $x_0$, $x_0^\ast$, and $b^\ast$.
Initialize using
    \begin{subequations}\begin{align}
    r_0 &= b - Ax_0 \\
    r_0^\ast &= b^\ast - x_0^\ast A \\
    p_0 &= r_0 \\
    p_0^\ast &= r_0^\ast.
    \end{align}\end{subequations}
Then loop over the following for $k = 0, 1, \ldots$
    \begin{subequations}\begin{align}
    \alpha_k &= r_k^\ast r_k / p_k^\ast A p_k \\
    x_{k+1} &= x_k + \alpha_k p_k \\
    x_{k+1}^\ast &= x_k^\ast + \overline{\alpha_k} p_k^\ast \\
    r_{k+1} &= r_k - \alpha_k A p_k \\
    r_{k+1}^\ast &= r_k^\ast - \overline{\alpha_k} p_k^\ast A \\
    \beta_k &= r_{k+1}^\ast r_{k+1} / r_k^\ast r_k \\
    p_{k+1} &= r_{k+1} + \beta_k p_k \\
    p_{k+1}^\ast &= r_{k+1}^\ast + \overline{\beta_k} p_k^\ast.
    \end{align}\end{subequations}

The residuals $r_k$ and $r_k^\ast$ satisfy
    \begin{subequations}\begin{align}
    r_k &= b - A x_k \\
    r_k^\ast &= b^\ast - x_k^\ast A,
    \end{align}\end{subequations}
    where $x_k$ and $x_k^\ast$ are the approximate solutions to
    \begin{subequations}\begin{align}
    Ax &= b \\
    x^\ast A &= b^\ast.
    \end{align}\end{subequations}



\begin{thebibliography}{99}
\bibitem{kelley} C. T. Kelley, 
    ``Iterative Methods for Linear and Nonlinear Equations'', 
    SIAM (1995).
\bibitem{pubdom} \url{http://en.wikipedia.org/wiki/Biconjugate_gradient_method}
\end{thebibliography}
\end{document}
